[
  {
    "objectID": "posts/Writing-Systems/index.html",
    "href": "posts/Writing-Systems/index.html",
    "title": "Writing Systems",
    "section": "",
    "text": "This is a series of blogs about linguistics. See also: Phonetics and Phonology Part 1 Morphology and Semantics Phonetics and Phonology Part 2"
  },
  {
    "objectID": "posts/Writing-Systems/index.html#introduction",
    "href": "posts/Writing-Systems/index.html#introduction",
    "title": "Writing Systems",
    "section": "Introduction",
    "text": "Introduction\nHave you ever wondered how kids sing their “abc’s” in different places? In some places, kids sing their “alpha beta gamma’s” or their “alif baa jiim’s”, which are pretty similar to our own. In some places, kids recite their “ka kha ga gha nga’s”, quite different from our own. Yet in other places, such a song about their “alphabet” would be physically impossible, with kids thinking this is a preposterous and cruel joke. All of these are tricks for remembering letters, which are part of writing systems. Writing systems are also known as orthographies. You already know at least one - the Latin alphabet, which you’re reading right now. Here’s an overview of how all the different writing systems work."
  },
  {
    "objectID": "posts/Writing-Systems/index.html#phonetic-orthographies",
    "href": "posts/Writing-Systems/index.html#phonetic-orthographies",
    "title": "Writing Systems",
    "section": "Phonetic Orthographies",
    "text": "Phonetic Orthographies\nIn phonetic writing systems, symbols are written based on sound. For example, in the Latin alphabet, the word “cat” is written with a “c” for the “c” sound, “a” for the “a” sound, and “t” for the “t” sound. Of course, it’s not always one-to-one, or even consistent, in English itself (cough cough, “cough”), but the main point still stands - it’s based on sound. In our language, you wouldn’t draw an image of a cat to represent a cat, you’d just write “cat” based on the sounds of the word. Here are the types of phonetic writing systems:\n\nAlphabets\nAlphabets work by having symbols for vowels and consonants, and they’re just written without really being treated as separate phonemes. These are arguably the simplest. Apart from the Latin alphabet, some others are Cyrillic, Greek, IPA, Georgian, Armenian, etc. Korean’s writing system, Hangeul, is kind of like an alphabet because it writes out its vowels and consonants, but it arranges each syllable into its own block. For example, the characters for “ba” would be 바, but the characters for “bo” would be 보. Furthermore, the characters for “bol” would be 볼. The word alphabet comes from the first to letters of Greek, alpha and beta.\n\n\nAbjads\nMost alphabets evolved from abjads. Abjads are like alphabets, except usually, most of the vowels aren’t written, and instead inferred from context. Only long vowels are written. Sometimes, for clarity, the short vowels may be written in with diacritics. For example, in the Quran, even though the Arabic script is an abjad, all the vowels were written so that there aren’t any misinterpretations. The word “abjad” comes from the first sounds of the four letters of the Arabic script, “a”, “b”, “j”, and “d”, which is cognate with Greek’s “alpha”, “beta”, “gamma”, “delta”.\nThe most prominent example of abjads are the orthographies of Semitic languages, which it works well for due to their triconsonantal root system, where consonants carry the meaning and vowels just modify the word. Hwvr, f y d ths n nglsh, t wll b prtt mssd p nd nt t ll ntllgbl.\n\n\nSyllabaries\nIn syllabaries, each symbol is a syllable, usually a pair of consonant and vowel. These work well for languages that don’t have crazy consonant clusters, like Japanese and Cherokee. However, if a language has a lot of possible syllables due to its large phonology, then there would be way too many symbols. \n\n\nAbugidas\nAbugidas, in my opinion, are by far the coolest type of phonetic orthography. How it works is that if you write a consonant on its own, it comes with a vowel after it. For example, the /k/ sound in Devanagari, क, is actually pronounced /kə/ because without altering the consonant, it comes with an /ə/ sound. To make it a different vowel, you add a diacritic. So, /ke/ would be के. And, to make it just /k/ with no vowel, you make it क्. The most popular abugidas are from India, including Devanagiri, Tamil, Bengali, etc. However, the word “abugida” comes from the Ge’ez script from Ethiopia, where the first four letters are “a”, “bu”, “gi”, and “da” (and this arrangement is cognate with Greek’s “alpha”, “beta”, “gamma”, “delta”, just like with the word “Abjad”). Another notable abugida, apart from Indian scripts, Southeast Asian scripts, and the Ge’ez script, is the Cree script. For changing the vowel, Cree doesn’t actually add a diacritic, but instead modifies the consonant through rotations. Cree is actually partially inspired by Devanagiri."
  },
  {
    "objectID": "posts/Writing-Systems/index.html#semantic-orthographies",
    "href": "posts/Writing-Systems/index.html#semantic-orthographies",
    "title": "Writing Systems",
    "section": "Semantic Orthographies",
    "text": "Semantic Orthographies\nInstead of writing symbols based on sound, some languages have orthographies where symbols are based on meaning.\n\nLogographies\nThe most common type of semantic writing system is a logography, and the most famous type of logography is Hanzi from Sinitic languages, which is also what Japanese’s Kanji is based on. For example, look at the symbol for “yi” in Mandarin or “ichi” in Japanese, which means “one”: “一” Now, for “er” in Mandarin or “ni” in Japanese, which means “two”: “二” So, sometimes, the symbol looks like what it means. However, many logographic symbols have evolved so much that they don’t seem to resemble what they mean. For example, “马” means “horse” (pronounced as “ma”). This looks nothing like a horse, but the ancient form of this symbol did.\nSometimes, it can be more complicated than this. In Mandarin, each symbol is generally one syllable as well, and sometimes symbols are made based on their relation with other semantic meanings, so you can build up symbols from smaller symbols. For example, the symbol for “forest” is: 森 while the symbol for “tree” is: 木\nAlso, symbols whose words sound similar can also be used for clarification. This is called the rebus principle. In Chinese, most characters contain a semantic part, like the symbol for “tree” that we saw, and also a phonetic part, that indicates how it’s pronounced. For example, the word for “mom”, “妈”, which is pronounced as “ma”, contains the symbol for “horse” (“马”) we saw earlier, since that too is pronounced as “ma”, even though “mom” has nothing to do with “horse” in terms of meaning. Only the left half actually semantically has anything to do with “mom”. Only a small percentage of Chinese characters are actually purely logographic - the vast majority follow the rebus principle. Therefore, it’s more accurate to call Hanzi a logo-syllabary, instead of a logography.\nEgyptian hieroglyphs were part logography, part phonetic as well. Some symbols worked just like the ancient form of Chinese, and others evolved into more of a syllabary or abjad.\n\n\nPictographies\nPictographies are kind of like logographies, except that it’s much more literal, without much evolution. Unlike Hanzi, where a lot of characters are derived from abstract connections, pictographies are literal depictions. This is likely where writing started first. An example is the Mayan script, where a lot of it was pictographic, but other symbols evolved into being logographic or syllabic. Also, even emojis are largely pictographic. However, pictographs, like logographs, still just represent words. \n\n\nIdeographies\nIdeographies are another type of semantic orthography, where an idea or a concept is directly written, without being tied to a specific sound. For example, some street signs symbols that don’t have words but rather symbols, some of the more ancient Hanzi initially, or even some of the more abstract emojis as well are ideographies."
  },
  {
    "objectID": "posts/Writing-Systems/index.html#evolution-of-writing",
    "href": "posts/Writing-Systems/index.html#evolution-of-writing",
    "title": "Writing Systems",
    "section": "Evolution of writing",
    "text": "Evolution of writing\nMany writing systems evolved from Egyptian hieroglyphs, to become primarily alphabets, abjads, or even abugidas. This is primarily in Europe, the Middle East, and North Africa, where Egypt indirectly influenced. For example, the European alphabets evolved from Ancient Greek’s alphabet (Latin through the Etruscan alphabet, which in turn came from the Ancient Greek alphabet), which evolved from Phoenecian’s abjad, which evolved from Egyptian hieroglyphs. Also, the Semitic abjads evolved from Phoenician’s abjad too. The Semitic languages are written from right to left, because it chose a direction to write from Phoenecian, which alternated. Ancient Greek also initially alternated directions line by line, but then chose to stick with only left to right, which all the European languages are written with now.\nIn India, as well as some parts of Southeast Asia, including Myanmar, Laos, and Thailand, the orthography (all abugidas) of those languages evolved from the Brahmi script, which was an abugida. It’s not certain where the Brahmi script is from, though it’s suggested to come from Egyptian hieroglyphs too, but it could also be an artificially created script that evolved naturally.\n\n\n\nAn example of Brahmi script found on The Iron Pillar in Qutub Minar, New Delhi, source\n\n\n\n\n\n\n\n\nCool fact to bore your friends with and be called a nerd!!!\n\n\n\nThe Brahmi script was initially all straight lines and sharp angles. In South India, Southeast Asia, and Sri Lanka, the letters look more curvy, because they were written on leaves, so sharp turns and straight lines would tear the leaves. However, in North India, the letters can be more angular and sharp, but still also some curves, because they were written on both leaves and stone.\n\n\nIn China and Japan, the characters are derived from the Ancient Chinese logographic characters. In Japan, there are three writing systems. Two are syllabaries derived from Chinese, and the other is the normal Chinese logography.\nIn both Koreas, Hangeul is used. Hangeul is an artificially created script that was made to address the high percentage of illiteracy, due to the fact that Chinese characters were way to hard for the common people to learn. However, Hangeul was designed to be very simple.\nThe Cherokee syllabary was made after one Native American looked at the European colonizers’ writing and tried to mimic the symbols to make a writing system for Cherokee. He’d seen both the Latin and Cyrillic scripts. However, the symbols in Cherokee that look similar to the European writing make completely different sounds, as he only knew the shape of the symbols, not what sound they made."
  },
  {
    "objectID": "posts/Writing-Systems/index.html#to-be-continued",
    "href": "posts/Writing-Systems/index.html#to-be-continued",
    "title": "Writing Systems",
    "section": "To Be Continued",
    "text": "To Be Continued\nOK, that phonetic writing system stuff was cool, but you need to know how sounds actually work to use the symbols that represent them. How does that work? Read the linked blogs to find out!"
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html",
    "title": "Phonetics and Phonology Part 2",
    "section": "",
    "text": "This is a series of blogs on linguistics. See also: Morphology and Semantics, Writing Systems, Phonetics and Phonology Part 1"
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#the-difference",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#the-difference",
    "title": "Phonetics and Phonology Part 2",
    "section": "The Difference",
    "text": "The Difference\nWhy did I use both [] and //? In short, [] is the actual scientific way of making as many distinctions as you want (phonetic transcription), so it doesn’t care about how specific languages perceive sounds. On the other hand, // is when you only care about the distinctions a specific language makes (phonemic transcription), so you’ll typically be less descriptive of how to pronounce that sound.\nFor example, Tamil doesn’t distinguish between voiced and voiceless consonants, so you could phonemically write the word for monkey as /kuɾaŋku/, even though phonetically, people actually pronounce it as something like [kuɾaŋɡʊ]. However, it doesn’t matter which of those distinctions are used, between [ʊ] and [u] or [k] and [g], since they’re perceived as being the same sound for Tamil speakers."
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#chill-ipa-characters",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#chill-ipa-characters",
    "title": "Phonetics and Phonology Part 2",
    "section": "Chill IPA Characters",
    "text": "Chill IPA Characters\nMany letters are just like in English. For example, why don’t you take a random guess at how /p/, /h/, /v/, /z/, /m/, /ɪ/ and /t/ are pronounced? Yes! They are pronounced like “p”, “h”, “v”, “z”, “m”, “ɪ” and “t”! BUT NOT LIKE “pea”, “aich”, “vee”, “zee”, “em”, “eye”, and “tea” - you need to hop off elementary school English if you said that."
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#troll-ipa-characters",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#troll-ipa-characters",
    "title": "Phonetics and Phonology Part 2",
    "section": "Troll IPA Characters",
    "text": "Troll IPA Characters\nHowever, there are some tricky ones too that are designed to deceive you. For example, what sounds do you think /j/, /x/, /ɑ/, /r/, /q/, /c/, and /ʔ/ make? No, it is not “jay”, “eks”, “aey”, “ar”, “kyoo”, “see”, and “hmmm?”. Instead, they make the voiced palatal approximant, voiceless velar fricative, unrounded open back vowel, voiced alveolar trill, voiceless uvular stop, voiceless palatal stop, and the glottal stop (glottal stop doesn’t have a voiced version, because it’s just the break between “uh” and “oh” in “uh-oh”)."
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#greek-ipa-letters",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#greek-ipa-letters",
    "title": "Phonetics and Phonology Part 2",
    "section": "Greek IPA Letters",
    "text": "Greek IPA Letters\nWe’re not done. There’s even Greek letters, which somehow we can never evade, regardless of the subject. /ɸ/, /θ/, /ʎ/, /ɤ/, and /ʊ/ are not phi, theta, lambda, gamma, and upside-down omega, but instead voiceless bilabial fricative, voiceless alveolar fricative, voiced palatal lateral approximant, back close-mid unrounded vowel, and then this random vowel that doesn’t know where it belongs. Phew. That torture is over. Or is it?"
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#diacritics-intro",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#diacritics-intro",
    "title": "Phonetics and Phonology Part 2",
    "section": "Diacritics Intro",
    "text": "Diacritics Intro\nWe still have the diacritics to go over, but this time I won’t subject you to the pain of looking at every diacritic. Instead, I’ll just say that the diacritics can alter a sound to fit any of the variations for consonants and vowels I talked about earlier, such as nasality, gemination, voicelessness, tone, and more."
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#visualization",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#visualization",
    "title": "Phonetics and Phonology Part 2",
    "section": "Visualization",
    "text": "Visualization\nTo see the visual representation of the IPA, and to actually listen to their sounds, click here.\nEach language has its phonology written, by placing the accurate symbols of all of its phonemes. For example, see the phonology of Fr*nch."
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part2/index.html#allophonic-variation",
    "href": "posts/Phonetics-and-Phonology-Part2/index.html#allophonic-variation",
    "title": "Phonetics and Phonology Part 2",
    "section": "Allophonic Variation",
    "text": "Allophonic Variation\nPhonemes can have different forms, so a single phoneme is more of a category. In the example for “monkey” in Tamil that we saw, the phoneme /k/ can mean [k] or [g] depending on the specific word. So, we’d say that the voicing of /k/ is NOT phonemic in Tamil - variations aren’t distinguished in meaning. [k] and [g] are considered allophones - different forms of the same phoneme.\nA more tangible example is tonality in English. As you (hopefully) know, English is not a tonal language. So, when someone speaks English with tones (for example, Steven’s dad in a Steven He video), it means the same thing as speaking normal English without any tones."
  },
  {
    "objectID": "posts/Nice puzzle to distract my ice skating/index.html",
    "href": "posts/Nice puzzle to distract my ice skating/index.html",
    "title": "Nice Puzzle to Distract My Ice Skating",
    "section": "",
    "text": "Yap\n I went ice skating today. I only go around once a year, which means I basically forget any progress I make in not looking like a monkey when I try each time. Just as I started to remotely get the hang of it for the third time in my life, I went to take a look at all the sponsors of the rink putting ads around it. One that caught my eye was, of course, from AoPS, which is what is shown on the image:\n“Does this problem make your brain FREEZE? Combine plus signs and eight 8’s to get 1,000.”\nWhile the problem was really simple just by brute forcing and intuition that even a six or seven year old (hehe) could solve it. The ad was, after all, placed to a general audience, not a bunch of orz IMO quals.\nHowever, out of my annoyingness, I sought to prove rigorously that the answer I got was the only possible one. I did this in my head, closing any logic holes, while at the same time figuring out how to ice skate properly (at least I was getting better at moving continuously, but my center of mass was still all over the place, so I ended up looking like a monkey anyway, but at least I never fell in the entirety of the hour I was there). So, without further ado, here’s the solution:\n\n\nActual Solution\nAssume there are \\(n\\) numbers being added up. \\(1 \\leq n \\leq 8\\), since there are at most eight numbers, which means there is one per digit, and at least one number, which is when all the digits are in one number.\nNow, we consider the sum mod \\(10\\). Since all \\(n\\) numbers have a units digit of \\(8\\) as it’s the only possible digit, and they add up to \\(1000\\), we get \\(8n \\equiv 0 \\pmod{10}\\). What this means is that \\(8n\\) is a multiple of \\(10\\), so it must also be a multiple of \\(5\\), since \\(10\\) is a multiple of \\(5\\). Since \\(8\\) is not a multiple of \\(5\\), but \\(8n\\) is a multiple of \\(5\\), then \\(n\\) must be a multiple of \\(5\\). The only multiple of \\(5\\) between \\(1\\) and \\(8\\) is \\(n = 5\\). Therefore, we have five numbers being added up.\nWe know that the maximum possible number is \\(888\\), because \\(8888\\) is already greater than \\(1000\\). If there are \\(2\\) or more \\(888\\)’s being added up, then the sum is also over \\(1000\\).\nWe wish to prove that there are not zero \\(888\\)’s either. Assume by contradiction that there actually are zero \\(888\\)’s, so the maximum possible number is just \\(88\\). Now, the maximum possible sum is just \\(4 \\cdot 88\\) which is far less than \\(1000\\) (this is the maximum because every \\(88\\) is the equivalent of adding eleven \\(8\\)’s). Also, to verify that in my head, instead of calculating, I just saw that \\(4 \\cdot 88 &lt; 4 \\cdot 100 = 400 &lt; 1000\\). Therefore, there must be exactly one \\(888\\) being added up.\nWe find the remaining four numbers must add up to \\(112\\). We can do a similar process to find the number of \\(88\\)’s that must be present. If there are no \\(88\\)’s, then the maximum sum is \\(32\\), not even close to \\(112\\). However, if there are two or more \\(88\\)’s, then the sum is already over \\(112\\). Therefore, there must be exactly one \\(88\\) present.\nSince there is exactly one \\(888\\) and exactly one \\(88\\), and there aren’t any higher possible numbers, the remaining three \\(8\\)’s are used with one \\(8\\) per number, filling up the remaining three numbers needed.\nTherefore, the final sum is \\(\\fbox{888 + 88 + 8 + 8 + 8 = 1000}\\). There are also \\(40\\) different ways to rearrange that same equation and keep it true, because it’s \\(2 \\cdot \\frac{5!}{3!}\\) (the \\(5!\\) is to rearrange all the five numbers being added, the \\(3!\\) is because we don’t care the order the individual \\(8\\)’s are in because it’s all the same, and the \\(2\\) is to make the sum on the left hand side or the right hand side)."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html",
    "href": "posts/Limit riLmOtOpU/index.html",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "",
    "text": "I first heard of constructed languages, or conlangs, when I heard of Esperanto. I did my research and found out that Esperanto is a terrible conlang for reaching its purpose, which was to promote world peace, since it was so Eurocentric. I also heard of Toki Pona, which combines many different languages, and is very simple due to its limited vocabulary. I wanted to make my own language, which is completely different from any other language, but has very simple grammar and vocabulary.\n\nLimit is my first conlang, and the majority of it was made before I knew any linguistics, so it understandably is really really awful. However, I guess it also means it’s less overcomplicated that my later conlangs."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#background",
    "href": "posts/Limit riLmOtOpU/index.html#background",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "",
    "text": "I first heard of constructed languages, or conlangs, when I heard of Esperanto. I did my research and found out that Esperanto is a terrible conlang for reaching its purpose, which was to promote world peace, since it was so Eurocentric. I also heard of Toki Pona, which combines many different languages, and is very simple due to its limited vocabulary. I wanted to make my own language, which is completely different from any other language, but has very simple grammar and vocabulary.\n\nLimit is my first conlang, and the majority of it was made before I knew any linguistics, so it understandably is really really awful. However, I guess it also means it’s less overcomplicated that my later conlangs."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#pronunciation-and-alphabet",
    "href": "posts/Limit riLmOtOpU/index.html#pronunciation-and-alphabet",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Pronunciation and Alphabet",
    "text": "Pronunciation and Alphabet\nLimit is written in Roman characters (for now, that is; we will introduce the real writing system later), but there are some rules to how the script works.\nConsonants are mostly lowercase, with the only exception being the letter “L”, which is always capitalized. Each consonant is a short sound. Short vowel sounds are lowercase, while long vowel sounds are uppercase. The language is almost phonetic; the pronunciation is exactly what is written, apart from a few rules. There are no silent letters.\nExamples for how to write and pronounce vowels:\n1. A short “O” like in the word “dog” would be lowercase.\n2. A long “A” like in the word “cake” would be uppercase.\n\n\n\n\n\n\nNote\n\n\n\nA quick note: The letter “U” (capital, which means long) does not contain a “y” sound in its pronunciation. It sounds like the o’s in the word “food” rather than the u in the word “university”.\nThere are some vowel and consonant sounds, of course, that cannot be produced with just the Roman alphabet. Therefore, the Limit alphabet contains some extra characters as well.\n\n\n\nExtra vowel sounds\n\n“%” sounds like “oo” in the word “book”.\n\n“@” sounds like “ow” like in the word “now”.\n\n“α” sounds like “ah” like in the word “drama”\n\n“&” sounds like “oi” like in the word “boy”.\n\n“*” sounds like “er” like in the word “singer”.\n\n“^” sounds like “on” like in the word “bon” in French (used for foreign loanwords).\n\n\n\nExtra consonant sounds\n\n“#” sounds like “zh”, like in the s in “measure”.\n\n“~” sounds like “th”, like in the word “think”.\n\n“β” sounds like the n in “song”.\n\n“!” sounds like “sh”, like in the word “shook”.\n\n\n\nOther adjustments to the alphabet\n\nTo make the “ar” or “or” sound, take “α” or “O” and add “*”.\n\nThe letter “c” is removed as its sound can be made from either “k” or “s”.\n\nThe letter “g” only can sound like the g in “gray”, and NOT sound like a j, like in “giraffe”.\n\nThe letter “j” is removed as any sound with a “j” can be made from “d#”.\n\nThe letter “q” is removed as any “qu” sound can be made with “kw” instead. “u”, however, is still perfectly safe as it is important for various vowel sounds.\n\nThe letter “x” is removed as any sound with an “x” can be made from either “ks” or “z”.\n\nThe letter “i” is pronounced as IPA [y], like “y” in Finnish or “u” in French.\nThe letter “L” is pronounced as IPA character of the same shape, like the “L” sound in the word “pull”.\nSlashes indicate that some specific words are grouped together.\n\nOne last thing: Limit is pronounced in either an Indian accent, except with the new pronunciation rules now given."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#pronouns-and-verbs",
    "href": "posts/Limit riLmOtOpU/index.html#pronouns-and-verbs",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Pronouns and Verbs",
    "text": "Pronouns and Verbs\nIf you have a pronoun and a verb, you have a simple sentence, so let us start here in our journey to learn real Limit words. There is no gender or verb conjugation for each pronoun in Limit, so this should be simple.\n\nPronouns\n\n\n\nEnglish\nLimit\n\n\n\n\nI\nLI\n\n\nYou\ndI\n\n\nHe/She/It\nkrI\n\n\nWe\nLId\n\n\nYou (plural)\ndId\n\n\nThey\nkrId\n\n\n\nTo make any of these pronouns formal, just add the prefix “yU”.\nTo make any of these pronouns insulting, just add the prefix “yoL”. If you are talking directly to someone, add the suffix “*” or “p*” for “dId” and “dI” respectively.\n\n\nVerbs\nPresent tense: Suffix “UL”\nPast tense: Suffix “Uz”\nFuture tense: Suffix “Uk”\nTo make something infinitive, remove the Uz/UL/Uk and add the suffix “Up”.\nTo negate a verb, add the suffix “O#U” or “#U” (depending on if it ends in a vowel or consonant) at the very end of the word.\n\nExample\n\nTo eat is “brikUp”.\n\n“I did not eat” would be “LI brikUzO#U”."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#syntax",
    "href": "posts/Limit riLmOtOpU/index.html#syntax",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Syntax",
    "text": "Syntax\nHere are some rules for sentence structure:\n\nThe order of a declarative sentence is Subject Object Verb.\n\nAdjectives always come after nouns, attached by a slash, without spaces.\n\nArticles come after nouns as well, also attached by a slash without spaces.\n\nIf there is an article and an adjective, the adjective comes after the article, and the adjective and the article are combined with slashes as well.\n\nAdverbs come before the verb or adjective that they modify, with a backslash with no spaces. If they modify an adjective, then it goes before the noun.\n\nThere is no distinction between proper and common nouns.\n\nPrepositions work the same way as in English.\n\nObjects, like in English, do not have to just be nouns but also any verb phrase that acts as a noun. Predicate adjectives and predicate nominatives also count as objects.\n\nPrepositional phrases (like “in the house”) count as objects as well.\n\nIndirect objects come right before direct objects.\n\nIndirect objects have the suffix “b”, or “Ob” if the word ends in a consonant.\n\nPredicate adjectives would actually be adverbs, as they modify the word “bLIvUp”.\nExamples:\nWrite “I like to eat” in Limit syntax and spelling.\nI tU Et LIk\nWrite “I give you a green box” in Limit syntax and spelling.\nI yUhU boks/A/grEn giv"
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#inflections",
    "href": "posts/Limit riLmOtOpU/index.html#inflections",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Inflections",
    "text": "Inflections\nTo inflect a word ending in O, make the final consonant of the inflection thing voiceless. For example, the plural of “riLmO” (monkey) will be “riLmOt” instead of “riLmOd” (riLmOd is not a real word for that reason). And, the possessive of “riLmO” will be “riLmOf” instead of “riLmOv”. Likewise, you’d say “riLmOkE” instead of “riLmOgE”, “riLmOpU” instead of “riLmObU”, and “riLmOsA” instead of “riLmOzA”."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#punctuation",
    "href": "posts/Limit riLmOtOpU/index.html#punctuation",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Punctuation",
    "text": "Punctuation\nCommas are written as “~”.\nPeriods are written as “-”.\nQuotation marks are written as “+” on either end of the quote.\nQuestion marks are just a word, that being “LE”."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#possession",
    "href": "posts/Limit riLmOtOpU/index.html#possession",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Possession",
    "text": "Possession\n\nPossessive pronouns/adjectives\n\n\n\nEnglish\nLimit\n\n\n\n\nMy/Mine\n“LIv”\n\n\nYour/Yours\n“krIv”\n\n\nHis/Her/Its\n“dIv”\n\n\nOur/Ours\n“LIdOv”\n\n\nYour/Yours (plural)\n“krIdOv”\n\n\nTheir/Theirs\n“dIdOv”\n\n\n\nHow to use a possessive noun for a person (for example, “That is Bob’s”):\nAdd “Ov” to the end of their name if it ends in a consonant, otherwise add “v”.\nIf the noun it modifies is plural, then add “Od” or “d”.\nExamples:\nMy books are big = “zO*pOd/LIvOd Lib blivUL”\nThe world is mine = “h%d/vU LIv blivUL”"
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#adjectives",
    "href": "posts/Limit riLmOtOpU/index.html#adjectives",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Adjectives",
    "text": "Adjectives\n\nArticles\nThe (singular): vU\nThe (plural): vUd\nIndefinite articles are implied if there is no article.\nRemember, the articles go after the noun and before another adjective, connected with a slash.\n\n\nDemonstratives\nThis: #%t\nThat: #at\nThose: #atOd\nThese: #%tOd\nDemonstratives go after the noun and before another adjective, connected with a slash, just like articles. If they are pronouns, then just treat them as pronouns instead.\n\n\nPlurals\nAdjectives have to agree with their noun in number. If the noun is singular, the adjectives will also be singular, and if the noun is plural, the adjectives will also be plural. To make a noun or an adjective plural, add a “d” if it ends in a vowel, and add an “Od” if it ends in a consonant.\nAdjective phrases also go after the noun, and just the first word of the adjective phrase will be combined with a slash to the noun.\nThere will also be a slash after the last word, if the adjective phrase is more than a single word."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#adverbs",
    "href": "posts/Limit riLmOtOpU/index.html#adverbs",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Adverbs",
    "text": "Adverbs\nTo turn an adjective into an adverb, add the prefix “.@”.\nThis is analogous to adding an “ly” at the end in English.\nAdverbs go right before the adjective, verb, or adverb that they modify.\nExample:\nThe word for “real” is “gOhO”, which means the word for “really” is “.@gOhO”.\nI am really good = “LI .@gOhO\\LimitO#U blivUL”"
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#sandhi-and-phonotactic-exceptions",
    "href": "posts/Limit riLmOtOpU/index.html#sandhi-and-phonotactic-exceptions",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Sandhi and Phonotactic Exceptions",
    "text": "Sandhi and Phonotactic Exceptions\n\nAny nasal followed directly by a vowel must be turned into a voiceless plosive of the same place of articulation\n\nFor example, the “m” in “Limit” is pronouned like the “pppp” in Proto-Zorbbbb.\n\nAny word ending with a consonant must be pronounced, after the consonant, with a monkey scream (a shriek of [ɐ] at the top of your lungs) if the consonant is voiced and a grunt (a very low-pitch, almost confused-sounding [œ]) if the consonant is voiceless. This additional noise is never written. Also, the final consonant is an ejective consonant.\nAny word starting with a vowel is pronounced with an “f” if the vowel is rounded and a “k” if the vowel is unrounded."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#numbers",
    "href": "posts/Limit riLmOtOpU/index.html#numbers",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Numbers",
    "text": "Numbers\nBase 10 is used, like normal.\nOne shorthand is implemented; whenever there is a 1 followed by a 0, it is written as θ.\nNumbers are written as normal Arabic numerals, but obviously they are said differently.\n0: “fA”\n1: “hα”\n2: “rO”\n3: “d#U”\n4: “yA”\n5: “.a”\n6: “brU”\n7: “,O”\n8: “we”\n9: “gI”\nTo get more numbers, you do the following:\nWrite each digit in the bigger number as the sum of each digit times a power of \\(10\\). For example, \\(243\\) would be \\(2 \\cdot 10^2+4 \\cdot 10^1+3 \\cdot 10^0\\).\nYou may know that “time ten to the power of” can also be written as “E”. In Limit, the symbol for this is ““=”, pronounced “sigmα”. Fun fact: this word also means “and”.\nYou would write the each place value from highest exponent to lowest.\nInstead of saying plus, you would just combine each term with a slash.\nUnits digits do not need to be written as a=0, just a.\nBack to the example of 243, in Limit you would say this as:\n2=2/4=1/3, and pronounce this as rO=rO/yA=hα/jU, and remember that = is pronounced as sigmα.\nFor negative numbers, add the suffix “yO”.\nFor ordinal numbers, add the suffix “hO”, after the “yO” if it is there.\nFor fractions, just use the postposition for “in”, which is the suffix “krA” or “OkrA”. There will be a space after that.\nYou can also just list each digit, but then you must indicate the decimal point when applicable, which is pronounced as “%nd” and is written as “*”.\nExample:\nYou would say “31.4th” as “jU\\(hα/hα/yA\\)hαyOhO”."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#questions",
    "href": "posts/Limit riLmOtOpU/index.html#questions",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Questions",
    "text": "Questions\nJust say the equivalent declarative sentence, then add “LE-” (which means question mark) to the end.\nPhrasing Examples:\n“Which one do you like?” -&gt; “You one/which like LE-”\n“How are you?” -&gt; “You how are LE-”\nHere’re the interrogative words:\nWhat:"
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#commands",
    "href": "posts/Limit riLmOtOpU/index.html#commands",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Commands",
    "text": "Commands\nState the equivalent in a future tense declarative sentence, and speak in 2nd person. Use the word for please, “hok”, at the end of the sentence, after the verb.\nPhrasing Examples:\n“Go to bed.” -&gt; “You will to bed go hok-”\nAlso, instead of saying “hok”, you can say “bOLOdOwO#O”, which is short for “bU LI dI wopLUkO#U”, which means something along the lines of “or I will kill you”.\nFun fact: the word “hok” is actually a loanword from Proto-Zorbbbb."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#prepositions",
    "href": "posts/Limit riLmOtOpU/index.html#prepositions",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Prepositions",
    "text": "Prepositions\nFirst, be sure to note that Limit uses a suffix for prepositions (so they are actually postpositions) like “in”, “at”, “during”, “of”, etc; this means anything that gives the position or time, not something that is being headed towards. This suffix is “gE”, or “OgE”, depending on if the word ends in a vowel or consonant.\nThere is also a suffix for something you are heading to. This includes words such as “before”, “to”, or “for”. The suffix is “bU”, or “ObU”, depending on if the word ends in a vowel or consonant.\nFinally, for something you are coming from, such as “from”, “after”, or “past”, the suffix will be either “zA” or “OzA”."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#general-rule-of-affixes",
    "href": "posts/Limit riLmOtOpU/index.html#general-rule-of-affixes",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "General Rule of Affixes",
    "text": "General Rule of Affixes\nIf the word ends in a consonant, and the suffix starts with a consonant, then put an “O” between them.\nIf the word ends in a vowel and the suffix starts with a consonant, or the other way around, then there is no need for an extra “O”.\nThis same intuition is to be applied to prefixes."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#conjunctions",
    "href": "posts/Limit riLmOtOpU/index.html#conjunctions",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Conjunctions",
    "text": "Conjunctions\nThe conjunctions (subordinating and coordinating, and also including relative pronouns) are:\nAnd: “=”\nOr: “bI”\nBut: “pA”\nSo: “skibidE”\nIf: “*m”\nNote for the next two: If these account for plural antecedents, then add a “d”.\nWho/That/Which (as a subordinate): “Ligmα”\nWhose: “Ligmαv”"
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#abbreviations",
    "href": "posts/Limit riLmOtOpU/index.html#abbreviations",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Abbreviations",
    "text": "Abbreviations\nJust like how there’s “ts pmo” for [censored], in Limit there are abbreviations too. Except, between consonants, you must add a “@”. For example, bU LI dI wopLUkO#U becomes bOLOdOwO#O. Also, negation affixes are important enough to count as their own word, so just put #O at the end if it is negated, like in this one."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#comparatives-and-superlatives",
    "href": "posts/Limit riLmOtOpU/index.html#comparatives-and-superlatives",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "Comparatives and Superlatives",
    "text": "Comparatives and Superlatives\nHow you would phrase this:\n[Subject] [Person, indirect form] [Attribute, locative form] [Win/Lose].\nThe [Person, indirect form] can be removed if the sentence is superlative.\nNote that the indirect form and the locative form both have a postpositional ending, so it may be confusing.\nThe attribute must be a verb. Nouns, adjectives, and adverbs must be reworded."
  },
  {
    "objectID": "posts/Limit riLmOtOpU/index.html#vocabulary-in-progress",
    "href": "posts/Limit riLmOtOpU/index.html#vocabulary-in-progress",
    "title": "Limit riLmOtOpU (Limit For Monkeys)",
    "section": "VOCABULARY (in progress)",
    "text": "VOCABULARY (in progress)\nAlright, now that you know all the grammar rules, let’s get into the vocabulary, so you can actually speak in Limit. There aren’t that many vocabulary words, instead you must take the basic vocabulary words and combine them. To negate a word (to make it opposite, basically) add the suffix “O#U” or “#U”, which works for verbs, adjectives, nouns, etc. The core vocabulary is mainly for neutral or negative connotation, and to get positive connotation, just negate it.\nNouns:\nPerson: nUvak\nMonkey: riLmO\nIgnorance: zO*p\nWorld: h%d\nNon-primate organism: tEkU\nTool: dOmα*\nName: Eskα\nAge: derUm\nEvilness: Limit\nTime: dUrin\nMotion: cU\nSize: pLevI\nColor: snib\nGoal: wOksO\nX-axis: dAb\nY-axis: vEb\nZ-axis: LUb\nNature: nuk\nQuantity: LodO\nFood: gla*nE\nSet: rimU\nAdjectives:\nEvil: Limit\nSmall: pLevI\nSlow: kU\nIncorrect: kIf\nOld: derUm\nUgly: t!Untab\nCold: mEkO\nLate: dUrin\nYucky: gla*nE\nBlack: snib\nBlue: mraflet\nYellow: krUgnip\nRed: yundrask\nLiquid: vUgis Weird: kitU\nVerbs (infinitive):\nHate: netAbUp\nGo: drekInUp\nUse: gUatUp Make: wopLUp\nNot know: zO*pUp Steal: prIksUp\nWant: kutlOrUp"
  },
  {
    "objectID": "posts/Cool-Problem-of-the-Month2/index.html",
    "href": "posts/Cool-Problem-of-the-Month2/index.html",
    "title": "Why would anyone in their right mind do it like this?",
    "section": "",
    "text": "Yapping\nI recently have had the hobby of going through past AMC and AIME problems and trying to see if I could think of any goofy solution that’s not on the list of solutions on the AoPS wiki.\nFor one problem in particular, 2023 AMC 10B #7 (yeah, it’s still in the single digits), I found a diabolical solution that no one would have wanted to waste their brain cells to completely formulate. It is super easy to do just using 4th grade knowledge of geometry, but when I see rotations in geometry, I immediately think of complex bashing.\n\n\n\nThe problem with answer choices\n\n\nI actually wrote this solution on AoPS wiki last month, but this crazy backstory deserves its own post for Cool Problem of the Month, and November was already taken. See the solution I wrote here.\nI tried to reword or explain some of the solution a bit more in depth, so you can rest assured that I didn’t just plainly plagiarize myself.\nThe complex and trig bashing itself isn’t even bad. If the problem was at least in the double digits, it would likely have been perfectly acceptable to complex bash and then trig bash. However, for a question #7, I’m probably going to be socially ostracized for having done so, especially when a nine year old obsessed with Tung Tung Tung Sahur can beat you to it.\nWithout further ado, here is the solution.\n\n\nSolution\nLet the center of the square be the origin of the complex plane. Since we only care about angles, and not sides, we can assume WLOG that \\(A\\) is at the point \\(-1+i\\). To rotate a complex number clockwise by an angle \\(\\theta\\), just multiply the number by \\(e^{-i \\theta}\\), so then we end up decreasing the argument (angle measured counterclockwise from the x-axis) by \\(\\theta\\) while preserving the magnitude. In this case \\(\\theta = 20^{\\circ}\\).\nSo, the point \\(A'\\) is at \\((-1+i)(e^{-i 20^{\\circ}}) = (-1+i)(\\cos( - 20^{\\circ}) + i \\sin( - 20^{\\circ}))\\) from Euler’s formula (the exponential form is just my shorthand way of remembering how to rotate complex numbers, but you could just go straight to polar form). Since cosine is an even function and sine is odd, this is the same as \\((-1+i)(\\cos(20^{\\circ}) - i \\sin(20^{\\circ}))\\). This expands to \\((-\\cos(20^{\\circ}) + \\sin(20^{\\circ})) + i(\\cos(20^{\\circ}) + \\sin(20^{\\circ}))\\).\nThe angle we want to find is just the direction of the vector from \\(A\\) to \\(A'\\). So, this is just \\(\\arctan(\\frac{\\Delta \\Im}{\\Delta \\Re})\\). \\(\\Delta \\Im = \\cos(20^{\\circ}) + \\sin(20^{\\circ}) - 1\\), while \\(\\Delta \\Re = -\\cos(20^{\\circ}) + \\sin(20^{\\circ}) + 1\\). Essentially, the problem is just boiled down to finding out how to trig bash \\(\\arctan(\\frac{\\cos(20^{\\circ}) + \\sin(20^{\\circ}) - 1}{-\\cos(20^{\\circ}) + \\sin(20^{\\circ}) + 1})\\).\nThe is an identity that \\(\\cos(A)+\\sin(A)=\\sqrt{2}\\sin(A+45^{\\circ})\\) and \\(\\cos(A)-\\sin(A)=\\sqrt{2}\\cos(A+45^{\\circ})\\). I’ll leave it as an exercise to the reader to verify that. So, if we use this as well as the fact that \\(\\sin(90^\\circ - x) = \\cos(x)\\), the fraction simplifies to \\(- \\frac{\\sqrt{2} \\cos(25^\\circ)-1}{\\sqrt{2} \\sin(25^\\circ)-1}\\). Substituting \\(\\sqrt{2}=\\frac{1}{\\cos(45^\\circ)}\\), this simplifies to \\(- \\frac{\\cos(25^\\circ)-\\cos(45^\\circ)}{\\sin(25^\\circ)-\\cos(45^\\circ)} = - \\frac{\\cos(25^\\circ)-\\cos(45^\\circ)}{\\sin(25^\\circ)-\\sin(45^\\circ)}\\).\nFinally, there is an identity that \\(\\cos(A)-\\cos(B)=-2\\sin(\\frac{A+B}{2})\\sin(\\frac{A-B}{2})\\), and that \\(\\sin(A)-\\sin(B)=2\\cos(\\frac{A+B}{2})\\sin(\\frac{A-B}{2})\\). Again, verifying it is an exercise for the reader. So, we now have \\(- \\frac{2 \\sin(35^\\circ)\\sin(10^\\circ)}{-2 \\cos(35^\\circ)\\cos(10^\\circ)}\\) = \\(\\tan(35^\\circ)\\), so \\(\\arctan(\\tan(35^\\circ)) = \\boxed{35^\\circ}\\), or option \\(B\\)."
  },
  {
    "objectID": "posts/Abstract Derivatives Part 1/index.html",
    "href": "posts/Abstract Derivatives Part 1/index.html",
    "title": "Abstract Derivatives Part 1 - Linear Operators",
    "section": "",
    "text": "A linear operator \\(L\\) is just anything that satisfies \\(L(ax+by)=aL(x)+bL(y)\\), for constants \\(a\\) and \\(b\\), and where addition and multiplication are defined. In other words, it is additive, and you can pull out constants from the inside.\nMatrices are where you probably saw this. If you transform a vector \\(x\\) by multiplying it with \\(A\\), do the same for a vector \\(y\\), and add the two new vectors, it’s the same as just adding them first and then transforming them. Also, if you scale \\(x\\) and \\(y\\) by some constants, it doesn’t matter if you did it before or after the transformation.\n\n\n\nDiagram showing the constants rule of linear transformations\n\n\n\n\n\nDiagram showing additivity of linear transformations"
  },
  {
    "objectID": "posts/Abstract Derivatives Part 1/index.html#slight-tangent-on-affine-transformations",
    "href": "posts/Abstract Derivatives Part 1/index.html#slight-tangent-on-affine-transformations",
    "title": "Abstract Derivatives Part 1 - Linear Operators",
    "section": "Slight Tangent on Affine Transformations",
    "text": "Slight Tangent on Affine Transformations\nTransformations of the form \\(y=mx+b\\) aren’t linear, but instead affine. Obviously not everything has to be a scalar, but instead can be about vectors and matrices where you have \\(f(\\vec{v}) = A \\vec{v} + \\vec{b}\\) for some matrix \\(A\\). Where are affine transformations actually used?\nA really common example is to get from one layer of a neural network to another, right before the activation function (more on that in the blog about neural networks). You multiply the vector of inputs in one layer by the matrix of weights and add to that the vector of biases to get the next layer pre-activation."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog about math, linguistics and AI."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nWhy pi is not, in fact, equal to 4\n\n\n\nCool Math Stuff\n\nCool Problem of the Month\n\n\n\nMost problems are either computational or proofs… but this one’s a disproof, which will hopefully spice up 2026 a little.\n\n\n\n\n\nJan 29, 2026\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nWhy would anyone in their right mind do it like this?\n\n\n\nCool Math Stuff\n\nCool Problem of the Month\n\n\n\nI found this problem that’s really easy but I had a really goofy solution.\n\n\n\n\n\nDec 16, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract Derivatives Part 1 - Linear Operators\n\n\n\nCool Math Stuff\n\n\n\nAnswering the following questions - Why are matrices considered linear? How can differentiation also be interpreted as this?\n\n\n\n\n\nNov 23, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nAbstract Derivatives Part 2 - Multivariable and Beyond\n\n\n\nCool Math Stuff\n\n\n\nHow can the idea of derivatives as linear operators be applied to more complex functions, like multivariable functions, vector-valued functions, and vector fields? How does this relate in the slightest to AI?\n\n\n\n\n\nNov 23, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nNice Puzzle to Distract My Ice Skating\n\n\n\nCool Math Stuff\n\nCool Problem of the Month\n\n\n\nLol I found this while ice skating and proved it as rigorously as possible in my head while simultaneously figuring out how to ice skate without looking like a monkey.\n\n\n\n\n\nNov 22, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nMorphology and Semantics\n\n\n\nCool Linguistics Stuff\n\n\n\nHow do languages organize different units of meaning in a word? What even is a word? How does meaning relate to writing and sound? All of these and more will be answered in this blog post.\n\n\n\n\n\nNov 12, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nPhonetics and Phonology Part 1\n\n\n\nCool Linguistics Stuff\n\n\n\nHow do sounds work? What are some features of sounds that different languages distinguish between or care about? How do you organize any sound biologically possible for humans? All these and more will be answered in this blog post.\n\n\n\n\n\nSep 14, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nPhonetics and Phonology Part 2\n\n\n\nCool Linguistics Stuff\n\n\n\nWe know how sounds work biologically, but how do linguists write and organize them?\n\n\n\n\n\nSep 14, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nWriting Systems\n\n\n\nCool Linguistics Stuff\n\n\n\nHow do different types of writing systems work? What are they evolved from, and how? All these and more will be answered in this blog post.\n\n\n\n\n\nAug 23, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nLimit riLmOtOpU (Limit For Monkeys)\n\n\n\nCool Linguistics Stuff\n\nCool Conlangs\n\n\n\n\n\n\n\n\n\nJul 3, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\n\n\n\n\n\n\n\nHistorical Linguistics Part 1: The Beginning of Linguistics\n\n\n\nCool Linguistics Stuff\n\nCool History Stuff\n\n\n\nHow did people suddenly start thinking about languages scientifically? What were the results of this primary branch of linguistics?\n\n\n\n\n\nJan 29, 2025\n\n\nVaishnav Sudarshan\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Abstract Derivatives Part 2/index.html",
    "href": "posts/Abstract Derivatives Part 2/index.html",
    "title": "Abstract Derivatives Part 2 - Multivariable and Beyond",
    "section": "",
    "text": "For single variable functions, we have a scalar in and a scalar out (one input, one output). You could technically think of it as 1D vectors, as we’ll do soon, but it’s basically the same thing.\nThe main thing we want to emphasize here is that for the derivative, we write it as differentials (\\(df = f'(x) dx\\)) instead of \\(\\frac{df}{dx}\\). In scalar functions, that works, but later on, it won’t make much sense to divide by a vector or a matrix.\nDerivatives, however, aren’t just limited to acting on single-variables to be linear operators. Here are some other types of derivatives.\n\n\nFor multivariable functions, we have many inputs in and one output out, where each input or output is a number. However, one interpretation of a vector is a list of numbers, so one could think of it as a vector in and a scalar out. What this means is that, for a 2D input space, \\(f(\\vec{x}) = f(x, y)\\). For any point, or vector, on the 2D \\(xy\\) plane, there is a \\(z\\) coordinate corresponding to that, to form some sort of surface.\nAgain, what we basically just mean in this case is that a vector has two or more dimensions, and a scalar has just one. If we think of a list of numbers as a vector, then we can do vector operations on it, but if we think of a single number as a vector, doing vector operations on it wouldn’t make a difference.\nAs for the derivative? It’s a vector, known as the gradient! It is the vector of partial derivatives with respect to each variable. The gradient will always have the same “shape” as the input space, because it also represents the direction of steepest ascent. It’s written as \\(\\nabla f = [...\\frac{\\partial f}{\\partial x_i}...]\\), where \\(x_i\\) is each component in the input.\n\n\nWhy? Because think about this way: in differential form, we can write \\(df = \\Sigma {(\\frac{\\partial f}{\\partial x_i} dx_i)}\\). Oh, the sum of the corresponding products? Reminds me of dot product! We can write this as \\(df = \\nabla f \\cdot \\vec{dx}\\). In terms of magnitudes, it can also be written as \\(df = |\\nabla f| |d\\vec{x}| \\cos(\\theta)\\). We can’t really do much with the given magnitudes to maximize \\(df\\).\nAt a particular input point \\(\\vec{x}\\), the magnitude of \\(\\nabla f\\) is going to be some constant when we’re dealing with anything but that input point and the function. Also, the magnitude of \\(d\\vec{x}\\) can’t be dealt with because it’s an infinitesimal, not an actual number. So, the only thing we can do to maximize \\(df\\) is the \\(\\cos(\\theta)\\) term. The maximum of \\(\\cos(\\theta)\\) is \\(1\\), when \\(\\theta = 0^\\circ\\). So, if \\(d\\vec{x}\\) is in the same direction as the gradient, the function increases the most.\nSimilarly, for \\(df\\) to be as low as possible, we want \\(\\cos(\\theta)\\) to be at its minimum (\\(-1\\)), which is when \\(\\theta = 180^\\circ\\), or when we move in the opposite direction as the gradient. Now, it is guaranteed that \\(df \\leq 0\\), because we’re dealing with just magnitudes (always nonnegative) times \\(-1\\).\nSo, we can do some local linearization. If we have some random point \\(\\vec{x}, f(\\vec{x})\\), we can change \\(\\vec{x}\\) by some small vector in the opposite direction of \\(\\nabla f\\). Where do we get that small vector? First, we normalize \\(-\\nabla f\\) by dividing by its magnitude to get a unit vector. Then, to scale it so that our local linearization approximation isn’t horrendous, we multiply this vector by a small scalar \\(\\mu\\), known as the learning step. We subtract add this to the original \\(\\vec{x}\\), and recalculate \\(f(\\vec{x})\\).\nFor more on this, visit my blog on neural networks.\n\n\n\n\nWhat about a vector output for a scalar input? This could be represented as a parametric equation, and the derivative would also be a vector. An example of this is in physics, with velocity and acceleration, both as a function of time. See the diagram for these in circular motion.\n\n\n\nDiagram showing the vector-valued functions of velocity and acceleration during circular motion\n\n\nTo demonstrate where more abstract derivatives can be used in this case, we can use parametric equations. Let the counterclockwise angle above the horizontal of the particle moving in a circle be \\(\\theta\\), and the radius be \\(r\\). From components, the direction of the position of the particle from the center is \\([\\cos(\\theta), \\sin(\\theta)]\\). Then, from geometry, since motion is perpendicular to the radius, the horizontal component of velocity (assuming constant speed \\(v\\)) is \\(-v \\sin(\\theta)\\), and the vertical component is \\(v \\cos(\\theta)\\). Given that \\(\\omega = d\\theta/dt\\) and \\(v=\\omega r\\), try to prove for yourself that the centripetal acceleration must have magnitude \\(\\frac{v^2}{r}\\) and point towards the center! The full solution is in my blog post about circular motion.\n\n\n\nAt this point, we are exiting the more basic types of functions, and finally, matrices come into use here. You may recall that a vector field is a kind of function where for every point (vector) in space, there is an output which is a vector that moves that input somewhere else. As a result, what we can get by reiterating this process can be modeled for something like fluid flow. Maybe you’ve seen some of these diagrams before. Typically, since varying magnitudes get out of hand when drawn on paper, each point has a vector of a fixed length showing just direction, and the magnitudes are portrayed through color, where the “hotter” the color, the longer the magnitude is.\n\n\n\nDiagram showing a vector field\n\n\nThe diagram even looks intuitive. At each point, it tells you where to go next, and by how much. Even if you had no idea about what vector fields are, that’s literally what is looks like.\nThe derivative is a matrix called the Jacobian. Each row is basically the gradient with respect to the entire input from each component of the output.\nIf you think of it as looking at each component of the input, the Jacobian is also the derivative of the entire output with respect to each input.\nSo, if the input space is a vector in \\(\\mathbb{R}^n\\) and the output space is a vector in \\(\\mathbb{R}^m\\), the Jacobian is a matrix with dimensions \\(m \\times n\\).\nNow, the derivative can be written as \\(d \\vec{f} = J d \\vec{x}\\) for the Jacobian \\(J\\). Now, this looks like a proper matrix multiplication with no scalars involved apart from doing things component-wise.\nOne cool thing now is that if \\(J\\) is a constant matrix, the vector field is just a normal linear transformation of \\(\\vec{f}(\\vec{x}) = J \\vec{x}\\), which is an interesting interpretation of linear transformations."
  },
  {
    "objectID": "posts/Abstract Derivatives Part 2/index.html#multivariable-functions",
    "href": "posts/Abstract Derivatives Part 2/index.html#multivariable-functions",
    "title": "Abstract Derivatives Part 2 - Multivariable and Beyond",
    "section": "",
    "text": "For multivariable functions, we have many inputs in and one output out, where each input or output is a number. However, one interpretation of a vector is a list of numbers, so one could think of it as a vector in and a scalar out. What this means is that, for a 2D input space, \\(f(\\vec{x}) = f(x, y)\\). For any point, or vector, on the 2D \\(xy\\) plane, there is a \\(z\\) coordinate corresponding to that, to form some sort of surface.\nAgain, what we basically just mean in this case is that a vector has two or more dimensions, and a scalar has just one. If we think of a list of numbers as a vector, then we can do vector operations on it, but if we think of a single number as a vector, doing vector operations on it wouldn’t make a difference.\nAs for the derivative? It’s a vector, known as the gradient! It is the vector of partial derivatives with respect to each variable. The gradient will always have the same “shape” as the input space, because it also represents the direction of steepest ascent. It’s written as \\(\\nabla f = [...\\frac{\\partial f}{\\partial x_i}...]\\), where \\(x_i\\) is each component in the input.\n\n\nWhy? Because think about this way: in differential form, we can write \\(df = \\Sigma {(\\frac{\\partial f}{\\partial x_i} dx_i)}\\). Oh, the sum of the corresponding products? Reminds me of dot product! We can write this as \\(df = \\nabla f \\cdot \\vec{dx}\\). In terms of magnitudes, it can also be written as \\(df = |\\nabla f| |d\\vec{x}| \\cos(\\theta)\\). We can’t really do much with the given magnitudes to maximize \\(df\\).\nAt a particular input point \\(\\vec{x}\\), the magnitude of \\(\\nabla f\\) is going to be some constant when we’re dealing with anything but that input point and the function. Also, the magnitude of \\(d\\vec{x}\\) can’t be dealt with because it’s an infinitesimal, not an actual number. So, the only thing we can do to maximize \\(df\\) is the \\(\\cos(\\theta)\\) term. The maximum of \\(\\cos(\\theta)\\) is \\(1\\), when \\(\\theta = 0^\\circ\\). So, if \\(d\\vec{x}\\) is in the same direction as the gradient, the function increases the most.\nSimilarly, for \\(df\\) to be as low as possible, we want \\(\\cos(\\theta)\\) to be at its minimum (\\(-1\\)), which is when \\(\\theta = 180^\\circ\\), or when we move in the opposite direction as the gradient. Now, it is guaranteed that \\(df \\leq 0\\), because we’re dealing with just magnitudes (always nonnegative) times \\(-1\\).\nSo, we can do some local linearization. If we have some random point \\(\\vec{x}, f(\\vec{x})\\), we can change \\(\\vec{x}\\) by some small vector in the opposite direction of \\(\\nabla f\\). Where do we get that small vector? First, we normalize \\(-\\nabla f\\) by dividing by its magnitude to get a unit vector. Then, to scale it so that our local linearization approximation isn’t horrendous, we multiply this vector by a small scalar \\(\\mu\\), known as the learning step. We subtract add this to the original \\(\\vec{x}\\), and recalculate \\(f(\\vec{x})\\).\nFor more on this, visit my blog on neural networks."
  },
  {
    "objectID": "posts/Abstract Derivatives Part 2/index.html#vector-valued-functions",
    "href": "posts/Abstract Derivatives Part 2/index.html#vector-valued-functions",
    "title": "Abstract Derivatives Part 2 - Multivariable and Beyond",
    "section": "",
    "text": "What about a vector output for a scalar input? This could be represented as a parametric equation, and the derivative would also be a vector. An example of this is in physics, with velocity and acceleration, both as a function of time. See the diagram for these in circular motion.\n\n\n\nDiagram showing the vector-valued functions of velocity and acceleration during circular motion\n\n\nTo demonstrate where more abstract derivatives can be used in this case, we can use parametric equations. Let the counterclockwise angle above the horizontal of the particle moving in a circle be \\(\\theta\\), and the radius be \\(r\\). From components, the direction of the position of the particle from the center is \\([\\cos(\\theta), \\sin(\\theta)]\\). Then, from geometry, since motion is perpendicular to the radius, the horizontal component of velocity (assuming constant speed \\(v\\)) is \\(-v \\sin(\\theta)\\), and the vertical component is \\(v \\cos(\\theta)\\). Given that \\(\\omega = d\\theta/dt\\) and \\(v=\\omega r\\), try to prove for yourself that the centripetal acceleration must have magnitude \\(\\frac{v^2}{r}\\) and point towards the center! The full solution is in my blog post about circular motion."
  },
  {
    "objectID": "posts/Abstract Derivatives Part 2/index.html#vector-fields",
    "href": "posts/Abstract Derivatives Part 2/index.html#vector-fields",
    "title": "Abstract Derivatives Part 2 - Multivariable and Beyond",
    "section": "",
    "text": "At this point, we are exiting the more basic types of functions, and finally, matrices come into use here. You may recall that a vector field is a kind of function where for every point (vector) in space, there is an output which is a vector that moves that input somewhere else. As a result, what we can get by reiterating this process can be modeled for something like fluid flow. Maybe you’ve seen some of these diagrams before. Typically, since varying magnitudes get out of hand when drawn on paper, each point has a vector of a fixed length showing just direction, and the magnitudes are portrayed through color, where the “hotter” the color, the longer the magnitude is.\n\n\n\nDiagram showing a vector field\n\n\nThe diagram even looks intuitive. At each point, it tells you where to go next, and by how much. Even if you had no idea about what vector fields are, that’s literally what is looks like.\nThe derivative is a matrix called the Jacobian. Each row is basically the gradient with respect to the entire input from each component of the output.\nIf you think of it as looking at each component of the input, the Jacobian is also the derivative of the entire output with respect to each input.\nSo, if the input space is a vector in \\(\\mathbb{R}^n\\) and the output space is a vector in \\(\\mathbb{R}^m\\), the Jacobian is a matrix with dimensions \\(m \\times n\\).\nNow, the derivative can be written as \\(d \\vec{f} = J d \\vec{x}\\) for the Jacobian \\(J\\). Now, this looks like a proper matrix multiplication with no scalars involved apart from doing things component-wise.\nOne cool thing now is that if \\(J\\) is a constant matrix, the vector field is just a normal linear transformation of \\(\\vec{f}(\\vec{x}) = J \\vec{x}\\), which is an interesting interpretation of linear transformations."
  },
  {
    "objectID": "posts/Historical-Linguistics-1/index.html",
    "href": "posts/Historical-Linguistics-1/index.html",
    "title": "Historical Linguistics Part 1: The Beginning of Linguistics",
    "section": "",
    "text": "Right now, I’m jumping around the circle of linguistics a little. The trend has been to go from smaller units of language to bigger ones, like sounds to words to sentences. However, I think that going to one of the outermost rings, with the units of languages and language families, is helpful for more context with the inner rings too.\nHistorical linguistics is basically how the field of linguistics started - it’s not like early linguists from the 1800’s were saying “Oh, cool, a new field just appeared with barely any competition! Let’s convert tokens to vectors in semantic space using trained embedding matrices!” On the contrary, they were just observing languages at a broader scale, and realized that there actually were patterns, trends, and rules about how they could be related. That’s when they realized that languages could be analyzed scientifically, and weren’t just random gobbledegook.\n\n\nThis first part is probably way too much information, but for the full context, here you go.\n\n\nAfter “la Reconquista” in Spain and Portugal in the late 1400’s, where the Catholics kicked out the Ummayad Caliphate from the Iberian peninsula, they realized that they didn’t have to worry about being religiously persecuted for not being Muslim anymore. So, they made boats and set sail, to religiously persecute the rest of the world for not being Catholic. This is why the year Colombus reached the Americas and immediately started doing terrible things to the people there is the same year that “la Reconquista” finished.\nAfter the Spanish and the Portuguese made sailing around the world popular, other Europeans joined in as well, like the Italians, the British, the Dutch, and the French. The thing that all these Europeans had in common is that they all spoke Indo-European languages.\nAll the Europeans mainly sought after India, as well as nearby places like Southeast Asia, to get some spices, probably because they were sick of bland food. However, there was one major problem with this: the Ottoman Empire occupied Turkiye as well as much of the area around it, and they didn’t let anyone cross; even if they did, they couldn’t cross by boat, since the Suez Canal hadn’t been invented yet. So, they had to sail all the way around Africa, which I doubt anyone these days would have the patience to do.\n\n\n\nUpon finally reaching India, many Europeans noticed similarities between the local Indian languages and their own European languages. You can also see this yourself - the Hindu god of fire’s name is “Agni”, and in English, we have the word “ignite”. “Ignite” is from Latin, which means the mostly Romance-speaking Europeans were likely able to notice this with Indian languages. This is where the concept of cognates came from - words in different languages that have a common ancestor.\nObservations like these led the Europeans to hypothesize that all ancient European languages like Latin or Greek came from Sanskrit. At this point, the Europeans were very confused. They used to think that all languages were descended from Hebrew. Lots of fringe theories emerged from the discovery of Indo-European languages, like the idea that Germanic tribes were from Persia. I will probably talk about these theories in a future post, but for now, let’s just say that they would all make you want to cry.\nFinally, a linguist from Germany figured out that Sanskrit, Latin, Greek, and many other languages were all descended from a common ancestor language, which he called “Proto-Indo-European”. To do this, he invented the comparative method, which is what we will talk about in the next post.\nThis is the modern family tree of Indo European languages\n\n\n\n\nYou have to be really careful about cognates, because it could either be that the words really are similar and semantically related, they are false friends (which means that the words just coincidentally sound similar but don’t have a common ancestor), or they are loanwords, which means that one language borrowed the word from another, but this does not imply relatedness or ancestry.\nAn example of loanwords is when Korean uses “keikeu” for “cake”, but this doesn’t mean that Korean is a Germanic language; instead, it just means that Korean had to borrow from English because they didn’t have a word for “cake” before and they were exposed to Anglophone culture.\nFalse friends are also especially sneaky. If you told a German friend “Ich will dir Gift geben”, which you think means “I want to give you gifts”, they would probably stop being friends with you and even call the authorities on you, because “Gift” in German means “poison”, and has no relation to the English word “gift”.\n\n\n\nTo understand the comparative method that will be covered in the next post, we need to first understand sound changes. For example, the word for “water” in Latin is “aqua”, but in French, which is descended from it, it’s “eau”. We know that “eau” and “aqua” are cognates, because “eau” isn’t a loanword and it isn’t gibberish. How could sound changes have caused this to happen?\nThe main idea is that the /kʷ/ in “aqua” gradually turned into a /w/, since the /k/ sound got weakened, so it sounded more like /awa/. After that, the /w/ sound got weakened as well, and it sounded more like /au/. Finally, the /au/ sound got weakened to /o/, which is how we got “eau” (French also preserves its older spelling, which means at one point it really did used to sound something like “eeyeawoo”).\nThese sound changes are pretty regular in a single language or language family, and at least have patterns cross-linguistically. For example, it’s more likely for a /tʃ/ to emerge from a /t/ that is near an /i/ or a /j/ than the other way around, because of a trend called palatalization, which is why in Portuguese, the word for “milk” is “leite”, but is pronouned “leiche” (Portuguese preserves historical spelling like French does).\nOne of the most famous sound changes is Grimm’s law. It states that in Germanic languages (the transition from Proto-Indo-European to Proto-Germanic, to be exact), the /p/ sound that is found in other Indo-European languages turned into an /f/ sound, the /t/ sound was turned into a /θ/ sound, and the /k/ sound was turned into an /h/ sound. That’s why in English, we say “father”, “three”, and “heart”, while in the Vatican, Latin speakers say “pater”, “tres”, and “cor”. A small note is that the “th” in “father” is actually a /ð/ sound, not a /θ/ sound, but that’s because of a subsequent sound change after being its voiceless version."
  },
  {
    "objectID": "posts/Historical-Linguistics-1/index.html#historical-context-of-historical-linguistics",
    "href": "posts/Historical-Linguistics-1/index.html#historical-context-of-historical-linguistics",
    "title": "Historical Linguistics Part 1: The Beginning of Linguistics",
    "section": "",
    "text": "This first part is probably way too much information, but for the full context, here you go.\n\n\nAfter “la Reconquista” in Spain and Portugal in the late 1400’s, where the Catholics kicked out the Ummayad Caliphate from the Iberian peninsula, they realized that they didn’t have to worry about being religiously persecuted for not being Muslim anymore. So, they made boats and set sail, to religiously persecute the rest of the world for not being Catholic. This is why the year Colombus reached the Americas and immediately started doing terrible things to the people there is the same year that “la Reconquista” finished.\nAfter the Spanish and the Portuguese made sailing around the world popular, other Europeans joined in as well, like the Italians, the British, the Dutch, and the French. The thing that all these Europeans had in common is that they all spoke Indo-European languages.\nAll the Europeans mainly sought after India, as well as nearby places like Southeast Asia, to get some spices, probably because they were sick of bland food. However, there was one major problem with this: the Ottoman Empire occupied Turkiye as well as much of the area around it, and they didn’t let anyone cross; even if they did, they couldn’t cross by boat, since the Suez Canal hadn’t been invented yet. So, they had to sail all the way around Africa, which I doubt anyone these days would have the patience to do.\n\n\n\nUpon finally reaching India, many Europeans noticed similarities between the local Indian languages and their own European languages. You can also see this yourself - the Hindu god of fire’s name is “Agni”, and in English, we have the word “ignite”. “Ignite” is from Latin, which means the mostly Romance-speaking Europeans were likely able to notice this with Indian languages. This is where the concept of cognates came from - words in different languages that have a common ancestor.\nObservations like these led the Europeans to hypothesize that all ancient European languages like Latin or Greek came from Sanskrit. At this point, the Europeans were very confused. They used to think that all languages were descended from Hebrew. Lots of fringe theories emerged from the discovery of Indo-European languages, like the idea that Germanic tribes were from Persia. I will probably talk about these theories in a future post, but for now, let’s just say that they would all make you want to cry.\nFinally, a linguist from Germany figured out that Sanskrit, Latin, Greek, and many other languages were all descended from a common ancestor language, which he called “Proto-Indo-European”. To do this, he invented the comparative method, which is what we will talk about in the next post.\nThis is the modern family tree of Indo European languages"
  },
  {
    "objectID": "posts/Historical-Linguistics-1/index.html#warning-about-cognates",
    "href": "posts/Historical-Linguistics-1/index.html#warning-about-cognates",
    "title": "Historical Linguistics Part 1: The Beginning of Linguistics",
    "section": "",
    "text": "You have to be really careful about cognates, because it could either be that the words really are similar and semantically related, they are false friends (which means that the words just coincidentally sound similar but don’t have a common ancestor), or they are loanwords, which means that one language borrowed the word from another, but this does not imply relatedness or ancestry.\nAn example of loanwords is when Korean uses “keikeu” for “cake”, but this doesn’t mean that Korean is a Germanic language; instead, it just means that Korean had to borrow from English because they didn’t have a word for “cake” before and they were exposed to Anglophone culture.\nFalse friends are also especially sneaky. If you told a German friend “Ich will dir Gift geben”, which you think means “I want to give you gifts”, they would probably stop being friends with you and even call the authorities on you, because “Gift” in German means “poison”, and has no relation to the English word “gift”."
  },
  {
    "objectID": "posts/Historical-Linguistics-1/index.html#sound-changes",
    "href": "posts/Historical-Linguistics-1/index.html#sound-changes",
    "title": "Historical Linguistics Part 1: The Beginning of Linguistics",
    "section": "",
    "text": "To understand the comparative method that will be covered in the next post, we need to first understand sound changes. For example, the word for “water” in Latin is “aqua”, but in French, which is descended from it, it’s “eau”. We know that “eau” and “aqua” are cognates, because “eau” isn’t a loanword and it isn’t gibberish. How could sound changes have caused this to happen?\nThe main idea is that the /kʷ/ in “aqua” gradually turned into a /w/, since the /k/ sound got weakened, so it sounded more like /awa/. After that, the /w/ sound got weakened as well, and it sounded more like /au/. Finally, the /au/ sound got weakened to /o/, which is how we got “eau” (French also preserves its older spelling, which means at one point it really did used to sound something like “eeyeawoo”).\nThese sound changes are pretty regular in a single language or language family, and at least have patterns cross-linguistically. For example, it’s more likely for a /tʃ/ to emerge from a /t/ that is near an /i/ or a /j/ than the other way around, because of a trend called palatalization, which is why in Portuguese, the word for “milk” is “leite”, but is pronouned “leiche” (Portuguese preserves historical spelling like French does).\nOne of the most famous sound changes is Grimm’s law. It states that in Germanic languages (the transition from Proto-Indo-European to Proto-Germanic, to be exact), the /p/ sound that is found in other Indo-European languages turned into an /f/ sound, the /t/ sound was turned into a /θ/ sound, and the /k/ sound was turned into an /h/ sound. That’s why in English, we say “father”, “three”, and “heart”, while in the Vatican, Latin speakers say “pater”, “tres”, and “cor”. A small note is that the “th” in “father” is actually a /ð/ sound, not a /θ/ sound, but that’s because of a subsequent sound change after being its voiceless version."
  },
  {
    "objectID": "posts/Morphology-and-Basic-Semantics/index.html",
    "href": "posts/Morphology-and-Basic-Semantics/index.html",
    "title": "Morphology and Semantics",
    "section": "",
    "text": "This is a series of blogs on linguistics. See also: Phonetics and Phonology Part 2, Writing Systems, Phonetics and Phonology Part 1"
  },
  {
    "objectID": "posts/Morphology-and-Basic-Semantics/index.html#confusing-introduction",
    "href": "posts/Morphology-and-Basic-Semantics/index.html#confusing-introduction",
    "title": "Morphology and Semantics",
    "section": "Confusing Introduction",
    "text": "Confusing Introduction\nPreviously, in orthographies, and phonetics and phonology, we’ve discussed sounds and their symbols. Now, we take a step further into meaning.\nNow, I want to emphasize that GENERALLY, sound and meaning are two separate things. This should make sense if you think about it - the meaning of “meaning” is just the idea tied to that word, and the sound is how that word is pronounced. There is nothing about the voiced bilabial nasal /m/ or the closed front unrounded vowel /i/ that makes the word “meaning” tied to the idea of “the idea tied to that word”. Note to reader: I do not apologize for provding confusing examples like this.\nIt may appear that there are exceptions. For example, the “wh” in English commonly indicates an interrogative word, like the five that are commonly referenced - “who”, “what”, “why”, “where”, “when”. However, there’s nothing about the labiovelar approximant that is inherently tied to interrogative words. Instead, that was just an arbitrary sound that happened to evolve this way. Instead of calling the “w” and the “h” as phonemes, we would call “wh” a unit that evolved out of a morpheme.\nA morpheme is loosely defined as a unit of meaning. You may wonder why we do not just use the term “words” to describe these. The answer is that “words” are quite hard to define - I mean, an app like Word or Google Docs might just measure the number of spaces and then add one, because that’s generally convenient in English, but what about something like Mandarin? Would every sentence in Mandarin be a single “word” because there aren’t any spaces? Such a question isn’t easy or even helpful. Therefore, we use morphemes instead.\nSince sound and meaning are independent, if your English teacher ever uses something like alliteration or rhyme to analyze a literary text, now you know that such an analysis is completely bogus. However, another possiblity also exists - your teacher is imagining how that sound is pronounced, and the manner and difficulty could add some “insightful” meaning (by which I mean that it’s definitely not intended to be that deep, and any such annotation is surely a stretch)."
  },
  {
    "objectID": "posts/Morphology-and-Basic-Semantics/index.html#types-of-morphology-across-languages",
    "href": "posts/Morphology-and-Basic-Semantics/index.html#types-of-morphology-across-languages",
    "title": "Morphology and Semantics",
    "section": "Types of Morphology Across Languages",
    "text": "Types of Morphology Across Languages\nThere are two continuums that determine how a language’s morphology works. The first measures how many morphemes are squished into a single word, and the second measures how clearly morphemes are separated.\n\nSpectrum 1\nThe first continuum ranges from one morpheme per word to multiple morphemes put together in a word to entire sentences fit in a word. In that order, the types of languages would be analytic, synthetic, and polysynthetic.\nAnalytic languages would be something like Mandarin - the sentence “I don’t drink tea” would be “我不喝茶” (“Wǒ bù hē chá”). Each word is its own morpheme here. Purely analytic languages don’t rely on inflection, or altering the form of words - instead, they rely on particles, which are words that are put next to other words to c hange the grammatical meaning.\nAs a sidenote, Google Translate clearly agrees that each character is a word in this case, as shown here.\n\n\n\nShows how Google Translate separates words in Pinyin\n\n\nSynthetic languages actually inflect more and rely less on particles. For example, Latin uses verb conjugations and noun declensions to reflect gender, number, case, person, and more. As a result, it doesn’t use articles, pronouns are optional, and there are less words in general. However, the entire sentence doesn’t get smushed together, and you can still tell apart subjects, verbs, and objects.\nPolysynthetic languages are like an extreme form of regular synthetic languages. A single word could contain the meaning of an entire sentence, combining morphemes of subjects, objects, and verbs. You may have seen super long words of some indigenous languages of the Eskimo-Aleut family in Greenland, Alaska, and Northern Canada (possibly on signs while playing GeoGuessr), and wondered what they were yapping. Now you know it’s not actual yap, just a normal sentence put together into a giant word.\nBut hold on, I thought words aren’t reliable measurements, so why do we use words here? Here, linguists use the practical working definition of a word, which is essentially something that makes sense being separated and independent. For example, morphemes like affixes would just be considered part of that word, but adjectives would be their own word (at least, that’s how English treats affixes and adjectives). So, it’s not purely based on how they’re written down, but there doesn’t necessarily need to be a clean line either. It’s just on average, how is the density of morphemes generally?\n\n\nSpectrum 2\nThe second continuum ranges from very clear distinction between morphemes to completely unique, unpredictable morphemes outlining certain features- agglutinative to fusional.\nAn agglutinative language like Turkish clearly separates all of its morphemes. Take the “word” (more like sentence) “Evlerimizdenmişsiniz”.\nBreakdown:\nEv → “house” (root noun)\n-ler → plural → Evler = “houses”\n-imiz → our → Evlerimiz = “our houses”\n-den → from → Evlerimizden = “from our houses”\n-miş → hearsay/past tense → Evlerimizdenmiş = “it seems it was from our houses”\n-siniz → second person plural → Evlerimizdenmişsiniz = “apparently you (all) were from our houses”\nHowever, look at fusional languages. In Spanish, “hablamos” means “we speak”, with the ending “-amos” indicating that the pronoun is first person, and also plural (“we”). If you just knew the first person ending that’s not plural (“-o”), then you still wouldn’t be able to derive the ending “-amos” from that.\n\n\nConsonantal Root Systems\nThere is a feature in languages where the root meaning is carried out through the consonants, and the vowels between them just modify the meaning. This is pretty much unseen outside of Afro-Asiatic langauges.\nMost famously, Semitic languages (a sub-branch of Afro-Asiatic) such as Arabic and Hebrew use the triconsonantal root system. In Arabic, for example, the root “k-t-b” has to do with writing and books. The word “kitab” means “book”, the word “katib” means “writer”, and the word “maktaba” means “library”.\nNote that the extra consonant “m” in “maktaba” is added, because Arabic itself isn’t purely consonantal, and uses affixes as well. Interestingly, the word “kitab” spread to many languages like Farsi or Hindi to mean “book”, but since those languages aren’t Semitic, they don’t think of “kitab” using the triconsonantal root system, but rather just as any other word.\nGermanic languages like English do just just a little. Primarily, this is shown in tense changes, like “sing” -&gt; “sung” -&gt; “sang”, or “run” -&gt; “run” -&gt; “ran” as opposed to something like “reprimand” -&gt; “reprimanded” -&gt; “reprimanded”, where the latter is regular."
  },
  {
    "objectID": "posts/Morphology-and-Basic-Semantics/index.html#semantics-intro",
    "href": "posts/Morphology-and-Basic-Semantics/index.html#semantics-intro",
    "title": "Morphology and Semantics",
    "section": "Semantics Intro",
    "text": "Semantics Intro\nSemantics is how we know how close words’ meanings are. Just from writing, it’s not always possible. For example, take the words “cat”, “dog”, and “catastrophe”. If you didn’t know English, you might guess that perhaps “cat” and “catastrophe” are close in meaning, but that’s not true, relative to “dog” (dog-lovers, I know you’ll strongly object and point out that cats are a form of catastrophes). In fact, “dog” and “cat” are actually more closely related to each other than to “catastrophe”, because they both are pets.\nIn a future blog post, I’ll talk exclusively about semantics, but for right now, I’ll talk about a specific aspect of it.\n\nApplications In AI\nNatural Language Processing (NLP) not only allows computers to get a sense of how meanings are related, but also provides us humans a nice way to visualize them.\nI will talk about meanings in the context of NLP more in my AI blogs, but for right now, I’ll provide a brief explanation. Basically, the AI user’s input is split into tokens, which are kind of like morphemes except that they don’t necessarily provide any grammatical meaning, instead just smaller meanings that are convenient for the AI to handle.\nEvery token in the language (in our case, English) is mapped to a vector (the specific details of how that works will be explained in a future blog post). These vectors’ angles’ closeness tries to portray semantic relations, but not exactly in the way you might think.\nFor example, synonyms and antonyms (words that mean similar things and words that mean the opposite, respectively) would not be pointing in completely opposite directions, but rather all approximately in a similar direction.\nThis is because, if you think it, “hot” and “cold” are still about temperatures, so they should point to a similar place. Something that has nothing to do with temperatures whatsoever, such as “skibidi” (although to be fair, “skibidi” has nothing to do with anything useful whatsoever), should point in the opposite direction. How exactly do the angles portray semantic closeness? The cosine of the angle does.\nIf the directions are close, then the cosine will be almost \\(1\\), but if the directions are near-opposite, then the directions will be \\(-1\\), so the higher number reflects that the meanings are closer.\nThe magnitudes of the vectors refer to how frequent or useful the words are, as well."
  },
  {
    "objectID": "posts/Morphology-and-Basic-Semantics/index.html#conclusion",
    "href": "posts/Morphology-and-Basic-Semantics/index.html#conclusion",
    "title": "Morphology and Semantics",
    "section": "Conclusion",
    "text": "Conclusion\nHow words arrange meanings is cool, but how do sentences arrange the words containing those meanings? Stay tuned to find out!"
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part1/index.html",
    "href": "posts/Phonetics-and-Phonology-Part1/index.html",
    "title": "Phonetics and Phonology Part 1",
    "section": "",
    "text": "This is a series of blogs about linguistics. See also: Morphology and Semantics, Writing Systems, Phonetics and Phonology Part 2"
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part1/index.html#consonants",
    "href": "posts/Phonetics-and-Phonology-Part1/index.html#consonants",
    "title": "Phonetics and Phonology Part 1",
    "section": "Consonants",
    "text": "Consonants\n\nPlace of Articulation\nPlace of articulation is essentially where in your mouth a sound is being made, whether it be deep in the throat (I’m looking at you, certain language primarily spoken in a country that borders Belgium, Germany, Luxembourg, Spain, Andorra, Monaco, Brazil, Suriname, and Italy) or front in the lips (like at the end of “dumb”). From deepest in the throat to closest in the lips, the most commonly referred to categories include glottal, pharyngeal, uvular, velar, retroflex, palatal, alveolar, dental, labiodental, and bilabial.\n\n\nManner of Articulation\nManner of articulation is how the sound is actually made. For example, there is a difference between “boron” and “moron”, even though both [b] and [m] are both “bilabial” sounds (one of the places of articulation mentioned above). Such manners of articulation include:\nFricatives (blowing air, like in the “sssss” sound in “snake”);\nPlosives (stopping the sound, like how in “dart”, pronouncing both the [d] and [t] can’t really be continued);\nNasals (where air comes out of your “nnnnn-nostrils”);\nTrills (where a sound is like a plosive but gets rolled, like the Spanish rolled r in “perro”);\nClicks (air is sucked in instead of pushed out, but unfortunately these cool sounds don’t exist in English, so I can’t provide an example you know how to pronounce. However, the name of the language Xhosa, (in)famous for having a lot of clicks, has a click in its name when not butchered as /kshosa/);\nand more.\n\n\nMore\nAnother important parameter is voicedness. Voicedness is just if the vocal cords are vibrating while the sound is being made. To understand this, look at [s] and [z], or [k] and [g] (as an exercise, find the voiceless version of [b]). In most languages, this has a really important distinction. For example, take “tie” and “die”. However, we usually just include this within manner of articulation, because we can’t put 3D charts on the iPad screen your face is glued to.\nConsonants are classified in other ways than just the two (or three) parameters, though. For example, there are pulmonic and non-pulmonic consonants. This just means if the sound is normal or a freak that doesn’t use air coming from the lungs. The only non-pulmonic consonants are implosives, ejectives, and clicks, and the rest are generally pulmonic. There is also sonorance or obstruence. Sonorance means the vocal tract is pretty open, like a vowel. As a result, most sonorants, which include nasals, approximants, and liquids, tend to be voiced. Obstruence means there is some friction or turbulence which make the sounds more easily voiceless and are noisier.\nAdditionally, there are more factors that can alter a consonant.\nIf you know Hindi, for example, you know how aspiration - putting extra air after a consonant - can impact a word and does matter, like in the words “पल” (pronounced like “pal”, meaning “moment”) and “फल” (pronounced like “phal”, meaning “fruit”).\nIn Russian, palatalization - making the consonant more like a palatal sound, which basically just makes it sound like “y” sound is at the end - is a very important factor too, with the words “мать” (pronounced like “maty” with the y being as in “yo” and not “corny”) meaning “mother” but “мат” (pronounced like “mat”) meaning a swear word.\nGemination - making a consonant pronounced for longer - is important in Finnish, because “tapa” means “way” or “manner”, but “tappa” means “kill”."
  },
  {
    "objectID": "posts/Phonetics-and-Phonology-Part1/index.html#vowels",
    "href": "posts/Phonetics-and-Phonology-Part1/index.html#vowels",
    "title": "Phonetics and Phonology Part 1",
    "section": "Vowels",
    "text": "Vowels\nEven in vowels, two main parameters affect how a vowel is made. While vowels are not that interesting in my opinion, they are definitely required, because without them you’d be speaking either gibberish or Polish. Vowels tend to be voiced because they are sonorant and made by vibrating the vocal cords.\n\n\n\n\n\n\nCool fact to bore your friends with and be called a nerd!!!\n\n\n\nThe reason why whispering is quiet is because what you’re really doing is not vibrating your vocal cords at all. Since vowels are inherently voiced, by whispering, you end up not being able to project your voice very much at all, as vowels are kind of the base of speech.\n\n\nThe parameters are backness (deep in the throat or front in the mouth, like the x-axis) and height (vertical position, but described using open or close, like the y-axis). A third binary parameter is there, roundedness, which is if your mouth is shaped like a circle or not. For example, the [i] sound is a closed front unrounded vowel.\nUnlike consonants, backness and height are continuous, and when you chart them, you get essentially a diagram of your mouth, called a vowel diagram.\nThere are many more variations to vowels that can be made now. For example, tone is a big factor in Mandarin. 糖 (táng) means sugar, but 汤 (tāng) means soup. Nasality (how much air comes out of the nostrils) is important in Fr*nch, as “a” means “has” but “an” means “year”. Rhoticity (how much the vowel sounds r-colored) makes the word “or” sound American when rhotic, but British when you only pronounce it like /oː/. Vowels can also have creaky or breathy voice (basically making it sound like a sheep or a zombie).\n\nVowel Harmony\nVowel harmony is a very cool property found in some languages.\nBasically, all vowels in a word must be made with the same frontness, and sometimes closedness or roundedness too.\n\nExample\nFor example, in Kazakh, the plural marker is either “-лаp” or “-лер” (/lar/ or /ler/) depending on if the root of the noun is front or back.\n“қала” (/qɑˈlɑ/), meaning “city”, uses back vowels, so “cities” is “қалалар”.\nHowever, “көл” (/kœl/) meaning “lake”, uses front vowels, so “lakes” is “көлдер”.\nKazakh also has roundedness vowel harmony."
  },
  {
    "objectID": "posts/Why Pi Is Not 4/index.html",
    "href": "posts/Why Pi Is Not 4/index.html",
    "title": "Why pi is not, in fact, equal to 4",
    "section": "",
    "text": "Yapping\n2025 is over! That was a once in a lifetime year. It was a perfect square, and the next one will come in 2116. Yet even that is not very cool number, because who cares about the square of 46? 45 is a much nicer number than that. 45 is a triangular number (sum of the first nine positive integers in this case), so 2025 is the square of this triangular number, which you may recall also means it’s the sum of the first nine cubes.\n\\(\\sum_{i=1}^{n} i^3 = \\frac{n^2 (n-1)^2}{4} = [\\frac{n(n-1)}{2}]^2 = (\\sum_{i=1}^{n} i)^2\\).\nAlso, since 2025 is the square of 45, it’s the sum of the first 45 odd numbers.\n2026, on the other hand, is unbelievably boring. Its prime factorization is literally just \\(2\\cdot 1013\\). So, for the first CPOTM of the year, I’ll try to make 2026 more interesting by writing a disproof insteda of a proof.\nIf you’ve seen it, then sorry. If not, here it is:\nIf you take a square of side length \\(r\\) and then keep on cutting the corners so you preserve the perimeter \\(8r\\), but the shape approaches a circle, then why can’t you say that \\(\\pi = 4\\), because the diameter is \\(2r\\)?\n\n\n\nThe fake proof\n\n\n\n\nSolution\nIt seems correct, doesn’t it? The area certainly converges, because after cutting off the corners over and over again, you get the same infinitesimal rectangles that could integrate just like a normal circle to get the same result.\nThe problem is that the perimeter doesn’t converge. The arc length of a curve from \\(t=a\\) to \\(t=b\\) where \\(x\\) and \\(y\\) are both functions of \\(t\\) is given by \\(\\int_a^b \\sqrt{(\\frac{dx}{dt})^2+(\\frac{dy}{dt})^2} dt\\). This is NOT the same as x(b)-x(a)+y(b)-y(a).\nAre you still not convinced that the perimeter doesn’t converge? Zoom in on an infinitesimal piece of the diagram, which is where the arc length of the circle is basically a line.\n\n\n\nWhen you zoom in to a single tiny step, ignoring nearby steps and exaggerating the nearby curvature so you can see it’s still a circle\n\n\nIn an infinitesimal piece, the limiting process just turns the square into a staircase-like shape, say of x-length \\(dx\\) and y-length \\(dy\\). On a tiny “step”, the perimeter is just \\(dx+dy\\). However, that tiny part of the circle is just a line that goes through the endpoints of the step, so the perimeter is \\(\\sqrt{dx^2+dy^2}\\).\nYou might argue that maybe we should zoom in even more, and the snapshot we took wasn’t the true final result. However, the arc of the circle is already basically a line. By cornering in the square even more, you’re just going to get another line going through a step, and the situation won’t change, so we have, in fact, zoomed in enough. To actually make the square match the circle in its perimeter, we must directly make the horizontal and vertical lines turn into diagonal lines, which changes the perimeter."
  }
]